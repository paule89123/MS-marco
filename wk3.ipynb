{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5a8b9-da11-4629-8c6d-156de9bb384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install word2vec\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120180e-9131-488c-a595-3bc970092d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import list_datasets\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import sentencepiece as sp\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "datasets_list = list_datasets()\n",
    "\n",
    "# get MS Marco\n",
    "ms_df_dict = load_dataset('ms_marco', 'v1.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb0726-3311-4d08-8935-60d2ae980394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine queries and passages into a single list\n",
    "texts = []\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    queries = ms_df_dict[split]['query']\n",
    "    passages = ms_df_dict[split]['passages']\n",
    "    texts.extend(queries)\n",
    "    for passage_dict in passages:\n",
    "        passage_texts = passage_dict['passage_text']\n",
    "        texts.extend(passage_texts)\n",
    "    # for passage_dict in passages:\n",
    "    #     passage_texts = [p['passage_text'] for p in passage_dict]\n",
    "    #     texts.extend(passage_texts)\n",
    "\n",
    "print(texts[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe6245-7762-490a-9051-a6a5fd79cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SentencePiece tokenizer\n",
    "vocab_size = 30000\n",
    "model_name = 'ms_marco_spm'\n",
    "spm_model = sp.SentencePieceTrainer.train(\n",
    "    sentence_iterator=iter(texts),\n",
    "    model_prefix=model_name,\n",
    "    vocab_size=vocab_size,\n",
    "    character_coverage=1.0,\n",
    "    model_type='unigram'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8cf36-bd7d-43d8-878f-b5301a198971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained SentencePiece model\n",
    "model_name = 'ms_marco_spm'\n",
    "spm_model_path = f\"{model_name}.model\"\n",
    "spm_model.save(spm_model_path)\n",
    "\n",
    "# # Load the trained SentencePiece model\n",
    "sp_model = sp.SentencePieceProcessor()\n",
    "sp_model.load(spm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7734f-0a35-4457-a248-1b39ea0f43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts using the trained SentencePiece model\n",
    "tokenized_texts = [sp_model.encode_as_pieces(text) for text in texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "embedding_size = 300\n",
    "window = 5\n",
    "min_count = 1\n",
    "workers = 4\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=embedding_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers\n",
    ")\n",
    "\n",
    "# Save the trained Word2Vec model\n",
    "word2vec_model_path = 'ms_marco_word2vec.model'\n",
    "word2vec_model.save(word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc501e1-dbb0-483a-9b9e-41050e54e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load('ms_marco_word2vec.model')\n",
    "\n",
    "# Create an embedding layer using the Word2Vec weights\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.wv.vectors), freeze=True)\n",
    "print(\"Embedding layer shape:\", embedding_layer.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b1b98-97a7-4983-995f-336f1c30e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_triples(split):\n",
    "    queries = []\n",
    "    pos_docs = []\n",
    "    neg_docs = []\n",
    "\n",
    "    num_samples = len(ms_df_dict[split])\n",
    "    print(\"num_samples\", num_samples)\n",
    "    for idx in tqdm(range(num_samples)):\n",
    "        query = ms_df_dict[split][idx]['query']\n",
    "        \n",
    "        relevant_passages = ms_df_dict[split][idx]['passages']['passage_text']\n",
    "        # if idx < 1:\n",
    "        #     print(\"relevant_passages\", relevant_passages)\n",
    "\n",
    "        # for p in passages:\n",
    "        #     if isinstance(p, dict) and 'passage_text' in p:\n",
    "        #         relevant_passages.append(p['passage_text'])\n",
    "\n",
    "        if relevant_passages:\n",
    "            pos_doc = random.choice(relevant_passages)\n",
    "\n",
    "            # Select a random negative passage from a different query\n",
    "            neg_idx = random.randint(0, num_samples - 1)\n",
    "            while neg_idx == idx:\n",
    "                neg_idx = random.randint(0, num_samples - 1)\n",
    "\n",
    "            neg_passages = ms_df_dict[split][neg_idx]['passages']['passage_text']\n",
    "            neg_passage = random.choice(neg_passages)\n",
    "            neg_doc = neg_passage\n",
    "\n",
    "            # if idx < 1:\n",
    "                # print(\"relevant_passages\", relevant_passages)\n",
    "                # print(\"pos_doc\", pos_doc)\n",
    "                # print(\"neg_doc\", neg_doc)\n",
    "            queries.append(query)\n",
    "            pos_docs.append(pos_doc)\n",
    "            neg_docs.append(neg_doc)\n",
    "\n",
    "    return queries, pos_docs, neg_docs\n",
    "\n",
    "train_queries, train_pos_docs, train_neg_docs = generate_triples('train')\n",
    "test_queries, test_pos_docs, test_neg_docs = generate_triples('test')\n",
    "validation_queries, validation_pos_docs, validation_neg_docs = generate_triples('validation')\n",
    "\n",
    "print(\"yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca27bd-c6b6-4ac5-a4a6-97e1a8f0d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_queries[0]\", train_queries[0])\n",
    "print(\"train_pos_docs[0]\", train_pos_docs[0])\n",
    "print(\"train_neg_docs[0]\", train_neg_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38a232-172e-4331-9081-c76b937724f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSMarcoDataset(Dataset):\n",
    "    def __init__(self, queries, pos_docs, neg_docs, tokenizer):\n",
    "        self.queries = queries\n",
    "        self.pos_docs = pos_docs\n",
    "        self.neg_docs = neg_docs\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.queries[idx]\n",
    "        pos_doc = self.pos_docs[idx]\n",
    "        neg_doc = self.neg_docs[idx]\n",
    "\n",
    "        query_tokens = self.tokenizer.encode(query)\n",
    "        pos_doc_tokens = self.tokenizer.encode(pos_doc)\n",
    "        neg_doc_tokens = self.tokenizer.encode(neg_doc)\n",
    "\n",
    "        return query_tokens, pos_doc_tokens, neg_doc_tokens\n",
    "\n",
    "def collate_fn(batch):\n",
    "    queries, pos_docs, neg_docs = zip(*batch)\n",
    "    queries = pad_sequence([torch.LongTensor(q) for q in queries], batch_first=True)\n",
    "    pos_docs = pad_sequence([torch.LongTensor(d) for d in pos_docs], batch_first=True)\n",
    "    neg_docs = pad_sequence([torch.LongTensor(d) for d in neg_docs], batch_first=True)\n",
    "    return queries, pos_docs, neg_docs\n",
    "\n",
    "train_dataset = MSMarcoDataset(train_queries, train_pos_docs, train_neg_docs, sp_model)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "query_tokens, pos_doc_tokens, neg_doc_tokens = train_dataset[0]\n",
    "print(\"Query tokens:\", query_tokens)\n",
    "print(\"Positive document tokens:\", pos_doc_tokens)\n",
    "print(\"Negative document tokens:\", neg_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12714c33-dc23-4345-8764-47cbb536223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = MSMarcoDataset(validation_queries, validation_pos_docs, validation_neg_docs, sp_model)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, hidden = self.gru(input)\n",
    "        return output[:, -1, :]\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_size):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.query_encoder = EncoderRNN(embedding_layer.embedding_dim, hidden_size)\n",
    "        self.doc_encoder = EncoderRNN(embedding_layer.embedding_dim, hidden_size)\n",
    "\n",
    "    def forward(self, query, doc):\n",
    "        query_emb = self.embedding(query)\n",
    "        doc_emb = self.embedding(doc)\n",
    "        query_enc = self.query_encoder(query_emb)\n",
    "        doc_enc = self.doc_encoder(doc_emb)\n",
    "        return query_enc, doc_enc\n",
    "\n",
    "hidden_size = 128\n",
    "model = TwoTowerModel(embedding_layer, hidden_size)\n",
    "\n",
    "def distance_function(query_enc, doc_enc):\n",
    "    return nn.functional.cosine_similarity(query_enc, doc_enc)\n",
    "\n",
    "def triplet_loss(query_enc, pos_doc_enc, neg_doc_enc, margin=1.0):\n",
    "    pos_distance = distance_function(query_enc, pos_doc_enc)\n",
    "    neg_distance = distance_function(query_enc, neg_doc_enc)\n",
    "    loss = nn.functional.relu(margin - pos_distance + neg_distance)\n",
    "    return loss.mean()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for query, pos_doc, neg_doc in train_dataloader:\n",
    "        query, pos_doc, neg_doc = query.to(device), pos_doc.to(device), neg_doc.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        query_enc, pos_doc_enc = model(query, pos_doc)\n",
    "        _, neg_doc_enc = model(query, neg_doc)\n",
    "        loss = triplet_loss(query_enc, pos_doc_enc, neg_doc_enc)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for query, pos_doc, neg_doc in validation_dataloader:\n",
    "            query, pos_doc, neg_doc = query.to(device), pos_doc.to(device), neg_doc.to(device)\n",
    "            query_enc, pos_doc_enc = model(query, pos_doc)\n",
    "            _, neg_doc_enc = model(query, neg_doc)\n",
    "            loss = triplet_loss(query_enc, pos_doc_enc, neg_doc_enc)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
